{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaned_DownloadVer_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJCo_fwkmM5z"
      },
      "source": [
        "# **Binformatics Machine Learning Project**\n",
        "\n",
        "Rekkab Gill\\\n",
        "rekkab@uoguelph.ca\\\n",
        "Date April 9 2021\\\n",
        "Dataset: Mice Expression Data from UCI Machine Learning repository\\\n",
        "\n",
        "\n",
        "NOTE: The code here pulls the UCI dataset from google drive, so in order to run the code you must mount your google drive and indicate the location of the dataset or alter the code to use your own way of obtaining the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-QAICqUKsir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f94cc46-338e-4009-f9e4-6ac2784b2730"
      },
      "source": [
        "!pip install -q sklearn\n",
        "\n",
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njfVlRwLLnTn"
      },
      "source": [
        "#put dataset into the dataframe\n",
        "dataSetAll = pd.read_csv('/content/drive/My Drive/School/UoGuelph/CIS6060 Bioinformatics/Project/the_dataset/mice.csv')\n",
        "pd.set_option('min_rows',200)\n",
        "pd.set_option('max_rows',2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHqwe4XrYdhW"
      },
      "source": [
        "dataSetAll.isnull().sum() #lets look at the overall count of missing vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih6UZ94daZUK"
      },
      "source": [
        "#drop the columns we don't need and those that are missing alot of data\n",
        "dataSetAll.drop('MouseID',axis=1,inplace=True)\n",
        "dataSetAll.drop('BAD_N',axis=1,inplace=True)\n",
        "dataSetAll.drop('BCL2_N',axis=1,inplace=True)\n",
        "dataSetAll.drop('H3AcK18_N',axis=1,inplace=True)\n",
        "dataSetAll.drop('pCFOS_N',axis=1,inplace=True)\n",
        "dataSetAll.drop('EGR1_N',axis=1,inplace=True)\n",
        "dataSetAll.drop('H3MeK4_N',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOUkJWQ2aalO"
      },
      "source": [
        "dataSetAll.isnull().sum() #lets look at the overall count of missing vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hdUh62LnwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349dfdb6-8762-4259-8542-68e4c3a4c0e4"
      },
      "source": [
        "#dorop any empty columns\n",
        "dataSetAll.dropna(how = 'any', axis = 0, inplace = True); #drop the row that has NA values\n",
        "print(dataSetAll.shape) #lets look at the shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1047, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSLa0XO7LoH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598599b8-2f33-4b29-e99c-628a66a16478"
      },
      "source": [
        "dataSetAll.ClassType = pd.Categorical(dataSetAll.ClassType)\n",
        "dataSetAll['ClassType'] = dataSetAll.ClassType.cat.codes\n",
        "\n",
        "#Hot Encode whats necssary and get dummy columns that have the encoding for each categorical column\n",
        "geno_dummies = pd.get_dummies(dataSetAll.Genotype)\n",
        "treat_dummies = pd.get_dummies(dataSetAll.Treatment)\n",
        "behav_dummies = pd.get_dummies(dataSetAll.Behavior)\n",
        "\n",
        "#Now add all the colums together \n",
        "dataSetAll = pd.concat([dataSetAll,geno_dummies,treat_dummies,behav_dummies], axis = 1)\n",
        "\n",
        "#Next remove the old categorical columns because we don't need them \n",
        "dataSetAll.drop(['Genotype','Treatment','Behavior'],axis=1,inplace=True)\n",
        "\n",
        "#We also want to avoid the dummy variable trap so we remove 1 dummy variable from each categorical colum we encoded, to avoid colinearity \n",
        "#Note: often libraries will have this built into the functions so you dont have to worry about this but its still good practice and good to understand.\n",
        "#This is for regression analysis but since we are doing classificaiton, it doesn't really matter but still good to see how to do:\n",
        "#For p dummy columns per category we want p-1 columns in our dataframe\n",
        "dataSetAll.drop(['Ts65Dn','Saline','S/C'],axis=1,inplace=True)\n",
        "\n",
        "print(dataSetAll.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   DYRK1A_N   ITSN1_N    BDNF_N     NR1_N  ...  ClassType  Control  Memantine  C/S\n",
            "0  0.503644  0.747193  0.430175  2.816329  ...          0        1          1    1\n",
            "1  0.514617  0.689064  0.411770  2.789514  ...          0        1          1    1\n",
            "2  0.509183  0.730247  0.418309  2.687201  ...          0        1          1    1\n",
            "3  0.442107  0.617076  0.358626  2.466947  ...          0        1          1    1\n",
            "4  0.434940  0.617430  0.358802  2.365785  ...          0        1          1    1\n",
            "\n",
            "[5 rows x 75 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tyf2HgQi5XAR"
      },
      "source": [
        "dataSetAll.dtypes\n",
        "print(dataSetAll['ClassType'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyn6eLrJLxZ6"
      },
      "source": [
        "#create the dataset with the columns we need only\n",
        "#get the output column\n",
        "dataSet_output = dataSetAll['ClassType']\n",
        "\n",
        "#get the input data and remove the output colummn\n",
        "dataSet_input = dataSetAll.drop('ClassType',axis=1)\n",
        "\n",
        "#change the data to numpy arrays\n",
        "dataSet_input = pd.DataFrame.to_numpy(dataSet_input)\n",
        "dataSet_output = pd.DataFrame.to_numpy(dataSet_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XmWolwJL0HA"
      },
      "source": [
        "#normalize the data \n",
        "scaler = preprocessing.StandardScaler()\n",
        "dataSet_input = scaler.fit_transform(dataSet_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaKb97e5n8ZK",
        "outputId": "4dd1de44-d817-4fcf-8441-94d8d5a04448"
      },
      "source": [
        "#apply the PCA technique to reduce dimentionality \n",
        "\n",
        "pca = PCA(n_components = 10)\n",
        "pca.fit(dataSet_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7aEyHxytSqP",
        "outputId": "268ca349-1e87-410c-8b30-4c32d63d4003"
      },
      "source": [
        "print(pca.explained_variance_ratio_)\n",
        "dataSet_input = pca.transform(dataSet_input)\n",
        "print(dataSet_input.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.26783464 0.16557931 0.10740818 0.07588458 0.04792166 0.04661429\n",
            " 0.03612721 0.02971117 0.02264531 0.02003235]\n",
            "(1047, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAr-wngRrHxA"
      },
      "source": [
        "# NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnWmigackVoW"
      },
      "source": [
        "**HOW TO DO K-FOLD CROSS VALIDATION FROM SCRATCH**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubita4RrXrRs"
      },
      "source": [
        "#lets create a useful function for returning model evaluations \n",
        "histories = [];\n",
        "def get_eval(model,inputSetTrain, test_input, outputSetTrain, test_output):\n",
        "  History = model.fit(inputSetTrain,outputSetTrain,epochs=100, validation_data= (test_input,test_output), verbose = 0,\n",
        "                    callbacks = [early_stopper])\n",
        "  histories.append(History.history)\n",
        "\n",
        "  predictions = model.predict(test_input)\n",
        "  predictions = np.argmax(predictions,axis=1)\n",
        "  #print(predictions)\n",
        "  prediction_accuracy = metrics.accuracy_score(test_output,predictions)\n",
        "  prediction_precision = metrics.precision_score(test_output, predictions, average=\"macro\")\n",
        "  prediction_recall = metrics.recall_score(test_output, predictions, average=\"macro\")\n",
        "\n",
        "  print('accuracy: ',prediction_accuracy)\n",
        "  print('precision: ',prediction_precision)\n",
        "  print('recall: ',prediction_recall)\n",
        "\n",
        "  # Calculate confusion matrix and print it\n",
        "  cm = metrics.confusion_matrix(test_output, predictions)\n",
        "  print(cm)\n",
        "  \n",
        "  return model.evaluate(test_input, test_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ9PHkk0UZuJ",
        "outputId": "9145314d-d561-47ad-f15a-fb70d54d4d71"
      },
      "source": [
        "#Lets run KFold cross validations\n",
        "\n",
        "folds = StratifiedKFold(n_splits = 5, shuffle=True,random_state=1)\n",
        "#folds = KFold(n_splits = 5, shuffle=True,random_state=1)\n",
        "\n",
        "for train_index, test_index in folds.split(dataSet_input,dataSet_output):\n",
        "  training_input = dataSet_input[train_index]\n",
        "  testing_input = dataSet_input[test_index]\n",
        "  training_output = dataSet_output[train_index]\n",
        "  testing_output = dataSet_output[test_index]\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  #the way you read the below is that it initially expects input in the shape you have listed below and then spits out the value to the left, i.e. 128. The activiation indicates what function to use in that layer\n",
        "  model.add(tf.keras.layers.Dense(7,input_shape=(10,),activation='relu')) #1st hidden layer        \n",
        "  #model.add(tf.keras.layers.Dense(4, activation='relu')) # 2nd hidden layer (3)\n",
        "  model.add(tf.keras.layers.Dense(8, activation = 'softmax'))  # output layer (4)\n",
        "\n",
        "\n",
        "  #compile the model\n",
        "  model.compile(Adam(learning_rate = 0.001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  #early stopper\n",
        "  early_stopper = EarlyStopping(monitor = 'val_loss',min_delta = 0.01, patience = 5)\n",
        "\n",
        "  print(get_eval(model,training_input, testing_input, training_output, testing_output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.9095238095238095\n",
            "precision:  0.911090240210156\n",
            "recall:  0.911656746031746\n",
            "[[21  3  0  0  4  0  0  2]\n",
            " [ 0 24  0  0  0  0  0  0]\n",
            " [ 0  0 30  0  0  0  0  0]\n",
            " [ 0  0  0 23  0  0  0  1]\n",
            " [ 1  0  0  0 24  2  0  0]\n",
            " [ 1  0  0  0  0 18  0  2]\n",
            " [ 0  0  1  0  0  0 26  0]\n",
            " [ 0  0  0  0  0  0  2 25]]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.9095\n",
            "[0.33228617906570435, 0.9095237851142883]\n",
            "accuracy:  0.9238095238095239\n",
            "precision:  0.9281620374684716\n",
            "recall:  0.9258763227513227\n",
            "[[27  3  0  0  0  0  0  0]\n",
            " [ 1 23  0  0  0  0  0  0]\n",
            " [ 0  0 29  0  0  0  1  0]\n",
            " [ 0  0  0 24  0  0  0  0]\n",
            " [ 6  0  0  0 18  3  0  0]\n",
            " [ 0  0  0  0  1 20  0  0]\n",
            " [ 0  0  1  0  0  0 26  0]\n",
            " [ 0  0  0  0  0  0  0 27]]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.9238\n",
            "[0.22405895590782166, 0.9238095283508301]\n",
            "accuracy:  0.9330143540669856\n",
            "precision:  0.9351005333298461\n",
            "recall:  0.9361772486772486\n",
            "[[27  3  0  0  0  0  0  0]\n",
            " [ 0 24  0  0  0  0  0  0]\n",
            " [ 0  0 28  0  0  0  2  0]\n",
            " [ 0  0  0 24  0  0  0  0]\n",
            " [ 3  0  0  0 22  2  0  0]\n",
            " [ 0  0  0  0  1 20  0  0]\n",
            " [ 0  0  3  0  0  0 24  0]\n",
            " [ 0  0  0  0  0  0  0 26]]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9330\n",
            "[0.22999757528305054, 0.9330143332481384]\n",
            "accuracy:  0.9617224880382775\n",
            "precision:  0.9633333333333334\n",
            "recall:  0.9636574074074074\n",
            "[[27  3  0  0  0  0  0  0]\n",
            " [ 2 22  0  0  0  0  0  0]\n",
            " [ 0  0 29  1  0  0  0  0]\n",
            " [ 0  0  0 24  0  0  0  0]\n",
            " [ 1  0  0  0 26  0  0  0]\n",
            " [ 0  0  0  0  0 21  0  0]\n",
            " [ 0  0  1  0  0  0 26  0]\n",
            " [ 0  0  0  0  0  0  0 26]]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.9617\n",
            "[0.15724395215511322, 0.9617224931716919]\n",
            "accuracy:  0.9043062200956937\n",
            "precision:  0.9089928587154974\n",
            "recall:  0.9057857651607653\n",
            "[[25  3  0  0  2  0  0  0]\n",
            " [ 0 23  0  0  0  1  0  0]\n",
            " [ 0  0 30  0  0  0  0  0]\n",
            " [ 0  0  0 24  0  0  0  0]\n",
            " [ 1  3  0  0 21  2  0  0]\n",
            " [ 0  0  0  0  2 19  0  0]\n",
            " [ 0  0  1  0  0  0 25  1]\n",
            " [ 0  0  4  0  0  0  0 22]]\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.9043\n",
            "[0.2670225501060486, 0.9043062329292297]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlTpEiz4kcUZ"
      },
      "source": [
        "**HERE WE DO THE SAME AS ABOVE BUT USING THE SKLEARN BUILT IN FUNCTIONALITY**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c1s75krkiLW",
        "outputId": "c15bdb2f-01a5-42a8-f835-d5d8c669f869"
      },
      "source": [
        "#early stopper function to use for prevent overfitting\n",
        "early_stopper = EarlyStopping(monitor = 'val_loss',min_delta = 0.01, patience = 5)\n",
        "\n",
        "def createNetwork():\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  #the way you read the below is that it initially expects input in the shape you have listed below and then spits out the value to the left, i.e. 128. The activiation indicates what function to use in that layer\n",
        "  model.add(tf.keras.layers.Dense(7,input_shape=(10,),activation='relu')) #1st hidden layer        \n",
        "  #model.add(tf.keras.layers.Dense(4, activation='relu')) # 2nd hidden layer (3)\n",
        "  model.add(tf.keras.layers.Dense(8, activation = 'softmax'))  # output layer (4)\n",
        "\n",
        "\n",
        "  #compile the model\n",
        "  model.compile(Adam(learning_rate = 0.001),\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "nn = KerasClassifier(build_fn = createNetwork, epochs = 100, verbose = 0)\n",
        "strat_fold= StratifiedKFold(n_splits=10, shuffle=True, random_state= 1) #the strat_fold function is good for making sure the same amount of classes are in each fold so we use that in the corss_val_Score, instead of the default which is just throwing a value in there.\n",
        "cross_val_score(nn,dataSet_input, dataSet_output, cv = strat_fold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmVsA_JarO9k"
      },
      "source": [
        "# SUPPORT VECTOR MACHINE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Svu2fyulHsVO",
        "outputId": "4960d04e-b477-42e5-dbb3-fbf82a623d20"
      },
      "source": [
        "#lets do a grid search for the best parameters\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "parameters = [\n",
        "        {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "        {'C': [1, 10, 100, 1000], 'kernel': ['rbf'],\n",
        "                'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
        "\n",
        "\n",
        "#split the dataset to get validation\n",
        "ds_input, validation_input, ds_output, validation_output = train_test_split(dataSet_input, dataSet_output, test_size=0.15, random_state = 1)\n",
        "\n",
        "#get the best parameters\n",
        "svm_search = GridSearchCV(SVC(),parameters,n_jobs=-1,verbose=1)\n",
        "svm_search.fit(validation_input,validation_output)\n",
        "best_params = svm_search.best_params_\n",
        "\n",
        "folds = StratifiedKFold(n_splits = 5, shuffle=True,random_state=1)\n",
        "\n",
        "for train_index, test_index in folds.split(ds_input,ds_output):\n",
        "  training_input = ds_input[train_index]\n",
        "  testing_input = ds_input[test_index]\n",
        "  training_output = ds_output[train_index]\n",
        "  testing_output = ds_output[test_index]\n",
        "\n",
        "  svm_model = SVC(**best_params)\n",
        "\n",
        "  svm_model.fit(training_input,training_output)\n",
        "\n",
        "  predictions = svm_model.predict(testing_input)\n",
        "\n",
        "  prediction_accuracy = metrics.accuracy_score(testing_output,predictions)\n",
        "\n",
        "  prediction_precision = metrics.precision_score(testing_output, predictions, average=\"macro\")\n",
        "  prediction_recall = metrics.recall_score(testing_output,predictions, average=\"macro\")\n",
        "\n",
        "  print('accuracy: ',prediction_accuracy)\n",
        "  print('precision: ',prediction_precision)\n",
        "  print('recall: ',prediction_recall)\n",
        "\n",
        "  # Calculate confusion matrix and print it\n",
        "  cm = metrics.confusion_matrix(testing_output, predictions)\n",
        "  print(cm)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.9887640449438202\n",
            "precision:  0.9903846153846154\n",
            "recall:  0.9880952380952381\n",
            "[[24  0  0  0  0  0  0  0]\n",
            " [ 2 19  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9831460674157303\n",
            "precision:  0.9850589225589226\n",
            "recall:  0.9839221014492754\n",
            "[[23  1  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  2  0  0  0 21  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9775280898876404\n",
            "precision:  0.9807692307692308\n",
            "recall:  0.9742063492063492\n",
            "[[24  0  0  0  0  0  0  0]\n",
            " [ 2 19  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  2 16  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9550561797752809\n",
            "precision:  0.9555567168338908\n",
            "recall:  0.9569967532467532\n",
            "[[24  1  0  0  0  0  0  0]\n",
            " [ 2 19  0  0  0  0  0  0]\n",
            " [ 0  0 23  0  0  0  2  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0 22  2  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  1  0  0  0 21  0]\n",
            " [ 0  0  0  0  0  0  0 23]]\n",
            "accuracy:  0.9887005649717514\n",
            "precision:  0.9891304347826086\n",
            "recall:  0.9895833333333333\n",
            "[[22  2  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0]\n",
            " [ 0  0 26  0  0  0  0  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0 23  0  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    1.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdwOefLZQHwu"
      },
      "source": [
        "# LOGISTIC REGRESSION MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5oPG2mzRBKs",
        "outputId": "b5f52c18-c6ed-4955-a204-05f5f4d74c0f"
      },
      "source": [
        "#lets do a grid search for the best parameters\n",
        "\n",
        "parameters = [\n",
        "        {'penalty': ['l2'],\n",
        "         'C': np.logspace(0,4,10),\n",
        "         'solver': ['lbfgs','newton-cg','liblinear','sag'],\n",
        "         'max_iter': [5000]\n",
        "        }]\n",
        "\n",
        "#split the dataset to get validation\n",
        "ds_input, validation_input, ds_output, validation_output = train_test_split(dataSet_input, dataSet_output, test_size=0.15, random_state = 1)\n",
        "\n",
        "#get the best parameters\n",
        "log_search = GridSearchCV(LogisticRegression(),parameters,n_jobs=-1,verbose=1)\n",
        "log_search.fit(validation_input,validation_output)\n",
        "best_params = log_search.best_params_\n",
        "\n",
        "folds = StratifiedKFold(n_splits = 5, shuffle=True,random_state=1)\n",
        "\n",
        "for train_index, test_index in folds.split(ds_input,ds_output):\n",
        "  training_input = ds_input[train_index]\n",
        "  testing_input = ds_input[test_index]\n",
        "  training_output = ds_output[train_index]\n",
        "  testing_output = ds_output[test_index]\n",
        "  \n",
        "\n",
        "  log_model = LogisticRegression(**best_params)\n",
        "\n",
        "  log_model.fit(training_input,training_output)\n",
        "\n",
        "  predictions = log_model.predict(testing_input)\n",
        "\n",
        "  prediction_accuracy = metrics.accuracy_score(testing_output,predictions)\n",
        "\n",
        "  prediction_precision = metrics.precision_score(testing_output,predictions, average=\"macro\")\n",
        "  prediction_recall = metrics.recall_score(testing_output,predictions, average=\"macro\")\n",
        "\n",
        "  print('accuracy: ',prediction_accuracy)\n",
        "  print('precision: ',prediction_precision)\n",
        "  print('recall: ',prediction_recall)\n",
        "\n",
        "  # Calculate confusion matrix and print it\n",
        "  cm = metrics.confusion_matrix(testing_output, predictions)\n",
        "  print(cm)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed:    5.4s\n",
            "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy:  0.9831460674157303\n",
            "precision:  0.9838056680161944\n",
            "recall:  0.9821428571428572\n",
            "[[24  0  0  0  0  0  0  0]\n",
            " [ 2 18  0  0  0  1  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9831460674157303\n",
            "precision:  0.9839221014492754\n",
            "recall:  0.9809027777777778\n",
            "[[23  1  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  1  0  0  0 16  1  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9719101123595506\n",
            "precision:  0.9764957264957266\n",
            "recall:  0.9672619047619048\n",
            "[[24  0  0  0  0  0  0  0]\n",
            " [ 2 19  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 21  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  3 15  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n",
            "accuracy:  0.9719101123595506\n",
            "precision:  0.9751157407407407\n",
            "recall:  0.9738833992094862\n",
            "[[22  3  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0]\n",
            " [ 0  0 25  0  0  0  0  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0 24  0  0  0]\n",
            " [ 0  0  0  0  0 18  0  0]\n",
            " [ 0  0  1  0  0  0 21  0]\n",
            " [ 0  0  1  0  0  0  0 22]]\n",
            "accuracy:  0.9717514124293786\n",
            "precision:  0.9702907218639485\n",
            "recall:  0.9717693236714975\n",
            "[[22  2  0  0  0  0  0  0]\n",
            " [ 0 21  0  0  0  0  0  0]\n",
            " [ 0  0 26  0  0  0  0  0]\n",
            " [ 0  0  0 20  0  0  0  0]\n",
            " [ 0  0  0  0 21  2  0  0]\n",
            " [ 0  0  0  0  1 17  0  0]\n",
            " [ 0  0  0  0  0  0 23  0]\n",
            " [ 0  0  0  0  0  0  0 22]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
